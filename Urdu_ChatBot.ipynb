{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIhkAtfW+zw4Q6Ck/Q2ilX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deviljerry/Urdu-Conversational-Chatbot-Transformer-with-Multi-Head-Attention/blob/main/Urdu_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mJlHTk2AwX8",
        "outputId": "494730b6-7c0d-4715-b424-a794a43399cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m988.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Collecting urduhack\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.19.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.37.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Collecting tf2crf (from urduhack)\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-datasets~=3.1 (from urduhack)\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (25.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.0.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.17.3)\n",
            "INFO: pip is looking at multiple versions of typer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading typer-0.19.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading typer-0.18.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.5-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.4-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.2-py3-none-any.whl.metadata (15 kB)\n",
            "INFO: pip is still looking at multiple versions of typer to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading typer-0.17.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.16.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.14.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.13.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.13.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.4-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting typer-slim==0.12.0 (from typer-slim[standard]==0.12.0->typer<1.0,>=0.12->gradio)\n",
            "  Downloading typer_slim-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting typer-cli==0.12.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading typer_cli-0.12.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "INFO: pip is looking at multiple versions of typer-slim to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting urduhack\n",
            "  Downloading urduhack-1.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading urduhack-1.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading urduhack-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading urduhack-1.0.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading urduhack-1.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "  Downloading urduhack-0.3.4-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting transformers~=2.10 (from urduhack)\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyconll~=2.2 (from urduhack)\n",
            "  Downloading pyconll-2.3.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting tokenizers==0.7.0 (from transformers~=2.10->urduhack)\n",
            "  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses (from transformers~=2.10->urduhack)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting urduhack\n",
            "  Downloading urduhack-0.3.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "INFO: pip is still looking at multiple versions of typer-slim to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.3.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.12/dist-packages (from urduhack) (2.19.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (3.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (3.15.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (0.5.3)\n",
            "  Downloading urduhack-0.3.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "INFO: pip is looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.2.7-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading urduhack-0.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading urduhack-0.2.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "  Downloading urduhack-0.2.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "  Downloading urduhack-0.2.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading urduhack-0.2.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading urduhack-0.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.1.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urduhack-0.1.4-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ca9fbad395a3e17eb5df2e055af94a5732d0a94389f60f79657e6cc204ad252c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: urduhack, portalocker, colorama, sacrebleu, rouge-score\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 rouge-score-0.1.2 sacrebleu-2.5.1 urduhack-0.1.4\n"
          ]
        }
      ],
      "source": [
        "# ===========================================\n",
        "# ğŸš€ Urdu Conversational Chatbot using Transformer (from scratch)\n",
        "# ===========================================\n",
        "\n",
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio nltk datasets sentencepiece sacrebleu rouge-score gradio urduhack kaggle\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from datasets import load_dataset\n",
        "import gradio as gr\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "import urduhack\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 1: Load Dataset from Kaggle (Fixed)\n",
        "# ===========================================\n",
        "\n",
        "# Set up Kaggle API key (upload kaggle.json to Colab first)\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_KEY\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and unzip dataset\n",
        "!kaggle datasets download -d muhammadahmedansari/urdu-dataset-20000 -p ./data\n",
        "!unzip -q ./data/urdu-dataset-20000.zip -d ./data\n",
        "\n",
        "# -------------------------------------------\n",
        "# ğŸ”¹ Load the main TSV dataset\n",
        "# -------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "# Try reading TSV file safely\n",
        "file_path = \"./data/final_main_dataset.tsv\"\n",
        "data = pd.read_csv(file_path, sep=\"\\t\", quoting=3, on_bad_lines='skip')\n",
        "\n",
        "# Inspect available columns\n",
        "print(\"Columns:\", data.columns.tolist())\n",
        "print(\"Sample rows:\")\n",
        "print(data.head(3))\n",
        "\n",
        "# -------------------------------------------\n",
        "# ğŸ”¹ Identify text columns\n",
        "# -------------------------------------------\n",
        "# Most likely columns: 'input', 'response', 'question', 'answer', etc.\n",
        "# You can adjust column names below once you see printed names.\n",
        "\n",
        "if \"input\" in data.columns and \"response\" in data.columns:\n",
        "    data = data[[\"input\", \"response\"]]\n",
        "elif \"question\" in data.columns and \"answer\" in data.columns:\n",
        "    data = data[[\"question\", \"answer\"]]\n",
        "else:\n",
        "    # Fallback: use first two columns\n",
        "    data = data.iloc[:, :2]\n",
        "\n",
        "data.columns = [\"input_text\", \"target_text\"]\n",
        "\n",
        "# Drop NaNs and sample subset to fit in Colab memory\n",
        "data = data.dropna().sample(20000, random_state=42)\n",
        "\n",
        "print(\"âœ… Loaded dataset with shape:\", data.shape)\n",
        "print(data.sample(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtoOs5G_A3hk",
        "outputId": "5a5570af-1873-4808-d3ca-fb9f199a8b8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/muhammadahmedansari/urdu-dataset-20000\n",
            "License(s): other\n",
            "urdu-dataset-20000.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace ./data/char_to_num_vocab.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/final_main_dataset.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/limited_wav_files/limited_wav_files/common_voice_ur_26562732.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/limited_wav_files/limited_wav_files/common_voice_ur_26562733.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/limited_wav_files/limited_wav_files/common_voice_ur_26562734.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "n\n",
            "Columns: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment']\n",
            "Sample rows:\n",
            "                                           client_id  \\\n",
            "0  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "1  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "2  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "\n",
            "                           path                            sentence  up_votes  \\\n",
            "0  common_voice_ur_31771683.mp3  Ú©Ø¨Ú¾ÛŒ Ú©Ø¨Ú¾Ø§Ø± ÛÛŒ Ø®ÛŒØ§Ù„ÛŒ Ù¾Ù„Ø§Ùˆ Ø¨Ù†Ø§ØªØ§ ÛÙˆÚº         2   \n",
            "1  common_voice_ur_31771684.mp3   Ø§ÙˆØ± Ù¾Ú¾Ø± Ù…Ù…Ú©Ù† ÛÛ’ Ú©Û Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨Ú¾ÛŒ ÛÙˆ         2   \n",
            "2  common_voice_ur_31771685.mp3       ÛŒÛ ÙÛŒØµÙ„Û Ø¨Ú¾ÛŒ Ú¯Ø²Ø´ØªÛ Ø¯Ùˆ Ø³Ø§Ù„ Ù…ÛŒÚº         2   \n",
            "\n",
            "   down_votes       age gender accents  variant locale  segment  \n",
            "0           0  twenties   male     NaN      NaN     ur      NaN  \n",
            "1           1  twenties   male     NaN      NaN     ur      NaN  \n",
            "2           0  twenties   male     NaN      NaN     ur      NaN  \n",
            "âœ… Loaded dataset with shape: (20000, 2)\n",
            "                                              input_text  \\\n",
            "12264  58fdc64b73679a0dea8d4bbcd1152d255d735fd0e48e2e...   \n",
            "16706  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "19418  923cf937f8ab28ff4dab01c2b2e29c2b4d4afbc5fb7544...   \n",
            "\n",
            "                        target_text  \n",
            "12264  common_voice_ur_31973247.mp3  \n",
            "16706  common_voice_ur_31822532.mp3  \n",
            "19418  common_voice_ur_31822470.mp3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 1: Load and Prepare Urdu Text Dataset\n",
        "# ===========================================\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"./data/final_main_dataset.tsv\"\n",
        "data = pd.read_csv(file_path, sep=\"\\t\", quoting=3, on_bad_lines='skip')\n",
        "\n",
        "print(\"Columns:\", data.columns.tolist())\n",
        "print(\"Sample sentences:\")\n",
        "print(data[\"sentence\"].head(5))\n",
        "\n",
        "# Use only the text column\n",
        "sentences = data[\"sentence\"].dropna().astype(str).tolist()\n",
        "\n",
        "# Simulate conversational pairs:\n",
        "# e.g. sentence[i] -> sentence[i+1]\n",
        "input_texts = sentences[:-1]\n",
        "target_texts = sentences[1:]\n",
        "\n",
        "# Build DataFrame\n",
        "data = pd.DataFrame({\n",
        "    \"input_text\": input_texts,\n",
        "    \"target_text\": target_texts\n",
        "})\n",
        "\n",
        "# Take random subset to avoid memory overload\n",
        "data = data.sample(20000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"âœ… Dataset prepared for chatbot training\")\n",
        "print(data.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcMT42v4Equk",
        "outputId": "f9259635-2a7a-412d-9257-505a3cf6d840"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment']\n",
            "Sample sentences:\n",
            "0                   Ú©Ø¨Ú¾ÛŒ Ú©Ø¨Ú¾Ø§Ø± ÛÛŒ Ø®ÛŒØ§Ù„ÛŒ Ù¾Ù„Ø§Ùˆ Ø¨Ù†Ø§ØªØ§ ÛÙˆÚº\n",
            "1                    Ø§ÙˆØ± Ù¾Ú¾Ø± Ù…Ù…Ú©Ù† ÛÛ’ Ú©Û Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨Ú¾ÛŒ ÛÙˆ\n",
            "2                        ÛŒÛ ÙÛŒØµÙ„Û Ø¨Ú¾ÛŒ Ú¯Ø²Ø´ØªÛ Ø¯Ùˆ Ø³Ø§Ù„ Ù…ÛŒÚº\n",
            "3                       Ø§Ù† Ú©Û’ Ø¨Ù„Û’ Ø¨Ø§Ø²ÙˆÚº Ú©Û’ Ø³Ø§Ù…Ù†Û’ ÛÙˆ Ú¯Ø§\n",
            "4    Ø¢Ø¨ÛŒ Ø¬Ø§Ù†ÙˆØ± Ù…ÛŒÚº Ø¨Ø·Ø® Ø¨Ú¯Ù„Ø§ Ø§ÙˆØ± Ø¯ÙÙˆØ³Ù’Ø±Ø§ Ø¢Ø¨ÛŒ Ù¾Ø±Ù†Ø¯Û Ø´...\n",
            "Name: sentence, dtype: object\n",
            "âœ… Dataset prepared for chatbot training\n",
            "                                  input_text  \\\n",
            "0                                ÛŒÛ Ø³Ø¨ ØªØ³Ù„ÛŒÙ…   \n",
            "1      Ø§ÙˆØ± ÙˆÛ ØªÙ… Ù¾Ø± Ø§Ù¾Ù†Û’ Ù†Ú¯Ø±Ø§Ù† Ù…Ù‚Ø±Ø± Ø±Ú©Ú¾ØªØ§ ÛÛ’   \n",
            "2      Ø³ÛŒØ§Ø³ÛŒ Ø­Ù‚ÙˆÙ‚ Ú©Û’ Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ú©Ùˆ Ø¬Ø§Ø¦Ø² Ù‚Ø±Ø§Ø± Ø¯ÛŒ   \n",
            "3                     Ù„ÙˆÚ¯ Ú†Ù„ Ú©Ø±Ú¯Ø¦Û’Ø§ÙˆØ±Ù…Ø±Ú©Ø±Ø¢Ø¦Û’   \n",
            "4  Ø­Ø¶Ø±Øª Ø¹Ù„ÛŒ Ø¨Ù† Ø­Ø³ÛŒÙ† Ø±Ø¶ÛŒ Ø§Ù„Ù„Û Ø¹Ù†Û Ø³Û’ Ø±ÙˆØ§ÛŒØª ÛÛ’   \n",
            "\n",
            "                                      target_text  \n",
            "0                  Ø§Ù†Ú¾ÛŒÚº ÛÛŒØ±Ùˆ Ú©Ø§ Ø¯Ø±Ø¬Û Ø¨Ú¾ÛŒ Ø¯ÛŒØ§ Ú¯ÛŒØ§  \n",
            "1            Ø§Ù†Ø³Ø§Ù† Ú©Û’ Ø§Ù†Ø¯Ø± Ø´Ø¹ÙˆØ± Ú©ÛŒ Ø¹Ø¯Ø§Ù„Øª Ù‚Ø§Ø¦Ù… ÛÛ’Û”  \n",
            "2  ÛØ± ÙØ±ÛŒÙ‚ Ø¯ÙˆØ³Ø±Û’ Ú©Û’ Ù„ÛŒÛ’ Ø³ÙÙØ§Ø±Ø´Ø§Øª Ú©ÛŒ ØªØ¬ÙˆÛŒØ² Ø¯ÛŒØªØ§ ÛÛ’  \n",
            "3                                Ø¨ØªØ§Ù†ÛŒ Ø´Ø±ÙˆØ¹ Ú©Ø±Ø¯ÛŒÚº  \n",
            "4           Û” Ø¬Ù†ÛÛŒÚº Ø¯Ø¨Ø§Ù†Û’ Ø³Û’ ÛŒÛ Ù¹Ú¾Ù†ÚˆÛŒ ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚºÛ”  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 2: Preprocessing (Final Fixed Version)\n",
        "# ===========================================\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download both required tokenizer models\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# âœ… Urdu normalization function (custom implementation)\n",
        "def normalize_urdu(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]', '', text)  # remove diacritics\n",
        "    text = re.sub(r'[Ø§Ø¢Ø¥Ø£Ù±]', 'Ø§', text)  # normalize Alef\n",
        "    text = re.sub(r'[ÙŠÙ‰Ø¦]', 'ÛŒ', text)    # normalize Yeh\n",
        "    text = re.sub(r'[Ú¾ÛÛ€Ú¾Ù°]', 'Û', text)  # normalize Heh\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  # keep only Urdu chars\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # clean spaces\n",
        "    return text\n",
        "\n",
        "# Apply normalization\n",
        "data[\"input_text\"] = data[\"input_text\"].astype(str).apply(normalize_urdu)\n",
        "data[\"target_text\"] = data[\"target_text\"].astype(str).apply(normalize_urdu)\n",
        "\n",
        "# âœ… Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "data[\"input_tokens\"] = data[\"input_text\"].apply(tokenize)\n",
        "data[\"target_tokens\"] = data[\"target_text\"].apply(tokenize)\n",
        "\n",
        "# âœ… Build vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "all_tokens = [token for tokens in data[\"input_tokens\"] + data[\"target_tokens\"] for token in tokens]\n",
        "vocab = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + sorted(set(all_tokens))\n",
        "vocab2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2vocab = {i: w for w, i in vocab2idx.items()}\n",
        "\n",
        "print(\"âœ… Preprocessing complete!\")\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"Sample tokens:\", data['input_tokens'].head(3).tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTk755_1A7zK",
        "outputId": "ac179559-53da-4e0e-e25b-6742339dfa26"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Preprocessing complete!\n",
            "Vocabulary size: 10897\n",
            "Sample tokens: [['ÛŒÛ', 'Ø³Ø¨', 'ØªØ³Ù„ÛŒÙ…'], ['Ø§ÙˆØ±', 'ÙˆÛ', 'ØªÙ…', 'Ù¾Ø±', 'Ø§Ù¾Ù†Û’', 'Ù†Ú¯Ø±Ø§Ù†', 'Ù…Ù‚Ø±Ø±', 'Ø±Ú©ÛØªØ§', 'ÛÛ’'], ['Ø³ÛŒØ§Ø³ÛŒ', 'Ø­Ù‚ÙˆÙ‚', 'Ú©Û’', 'Ù…Ø·Ø§Ù„Ø¨Ø§Øª', 'Ú©Ùˆ', 'Ø¬Ø§ÛŒØ²', 'Ù‚Ø±Ø§Ø±', 'Ø¯ÛŒ']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 3: Dataset & Dataloader\n",
        "# ===========================================\n",
        "\n",
        "MAX_LEN = 40\n",
        "\n",
        "def encode(tokens):\n",
        "    tokens = [\"<sos>\"] + tokens[:MAX_LEN-2] + [\"<eos>\"]\n",
        "    ids = [vocab2idx.get(t, vocab2idx[\"<unk>\"]) for t in tokens]\n",
        "    return ids + [vocab2idx[\"<pad>\"]] * (MAX_LEN - len(ids))\n",
        "\n",
        "class UrduChatDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.inputs = df[\"input_tokens\"].tolist()\n",
        "        self.targets = df[\"target_tokens\"].tolist()\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(encode(self.inputs[idx])), torch.tensor(encode(self.targets[idx]))\n",
        "\n",
        "dataset = UrduChatDataset(data)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32)"
      ],
      "metadata": {
        "id": "tpYsWpGHA_Tv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 4: Transformer Model (Fixed for Batch-First + Mask Shape)\n",
        "# ===========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, d_model]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerChatbot(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=2, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # âœ… batch_first=True is critical here\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, n_heads, dim_feedforward=512, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model, n_heads, dim_feedforward=512, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate autoregressive mask for decoder to prevent peeking ahead.\"\"\"\n",
        "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "        return mask  # shape: [sz, sz]\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src, trg: [batch, seq_len]\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        trg_emb = self.embedding(trg) * math.sqrt(self.d_model)\n",
        "\n",
        "        src_emb = self.pos_encoder(src_emb)\n",
        "        trg_emb = self.pos_encoder(trg_emb)\n",
        "\n",
        "        # âœ… Create target mask dynamically per batch\n",
        "        tgt_seq_len = trg_emb.size(1)\n",
        "        tgt_mask = self._generate_square_subsequent_mask(tgt_seq_len).to(trg.device)\n",
        "\n",
        "        # Encoder + Decoder\n",
        "        memory = self.encoder(src_emb)\n",
        "        output = self.decoder(trg_emb, memory, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Output projection\n",
        "        return self.fc_out(output)\n"
      ],
      "metadata": {
        "id": "qgi8msaMBCDX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 5: Training\n",
        "# ===========================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerChatbot(len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab2idx[\"<pad>\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, trg in tqdm(train_loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, len(vocab)), trg[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"urdu_chatbot.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2UE00hMBF1N",
        "outputId": "2f1a20ce-2b37-4b59-8042-a38d055806ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:52<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.4982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:09<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 5.5674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:18<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 4.9248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [12:12<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 4.3796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [11:54<00:00,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 3.8975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 6: Evaluation\n",
        "# ===========================================\n",
        "\n",
        "def evaluate_bleu(model, loader):\n",
        "    model.eval()\n",
        "    refs, hyps = [], []\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            src = src.to(device)\n",
        "            output = model(src, trg[:, :-1].to(device))\n",
        "            pred = output.argmax(-1)\n",
        "            for i in range(pred.size(0)):\n",
        "                ref = [idx2vocab[t.item()] for t in trg[i] if t.item() not in [0, 1, 2]]\n",
        "                hyp = [idx2vocab[t.item()] for t in pred[i] if t.item() not in [0, 1, 2]]\n",
        "                refs.append([' '.join(ref)])\n",
        "                hyps.append(' '.join(hyp))\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
        "    print(f\"BLEU: {bleu.score:.2f}\")\n",
        "\n",
        "evaluate_bleu(model, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rbN0t50BKIP",
        "outputId": "7140f00d-99ca-40aa-802f-28d117e4d366"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU: 1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ğŸ”¹ Step 7: Gradio Chat Interface\n",
        "# ===========================================\n",
        "\n",
        "def generate_reply(prompt):\n",
        "    model.eval()\n",
        "    tokens = tokenize(normalize_urdu(prompt))\n",
        "    ids = torch.tensor([encode(tokens)]).to(device)\n",
        "    trg = torch.tensor([[vocab2idx[\"<sos>\"]]]).to(device)\n",
        "    for _ in range(MAX_LEN):\n",
        "        out = model(ids, trg)\n",
        "        next_token = out.argmax(-1)[:, -1]\n",
        "        trg = torch.cat([trg, next_token.unsqueeze(0)], dim=1)\n",
        "        if next_token.item() == vocab2idx[\"<eos>\"]:\n",
        "            break\n",
        "    result = [idx2vocab[i.item()] for i in trg[0]][1:-1]\n",
        "    return \" \".join(result)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_reply,\n",
        "    inputs=gr.Textbox(label=\"ğŸ—¨ï¸ Urdu Input\", placeholder=\"Ø§Ù¾Ù†Ø§ Ø³ÙˆØ§Ù„ ÛŒÛØ§Úº Ù„Ú©Ú¾ÛŒÚº...\", rtl=True),\n",
        "    outputs=gr.Textbox(label=\"ğŸ¤– Chatbot Reply\", rtl=True),\n",
        "    title=\"Urdu Transformer Chatbot\"\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "qQGDu5zABMso",
        "outputId": "d98d3f75-dbff-4e08-8df4-44344d484eab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8aa1a0523f28bd041e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8aa1a0523f28bd041e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}