{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIhkAtfW+zw4Q6Ck/Q2ilX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deviljerry/Urdu-Conversational-Chatbot-Transformer-with-Multi-Head-Attention/blob/main/Urdu_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mJlHTk2AwX8",
        "outputId": "494730b6-7c0d-4715-b424-a794a43399cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m988.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Collecting urduhack\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.19.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.37.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Collecting tf2crf (from urduhack)\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-datasets~=3.1 (from urduhack)\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (25.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.0.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.17.3)\n",
            "INFO: pip is looking at multiple versions of typer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading typer-0.19.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading typer-0.18.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.5-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.4-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.2-py3-none-any.whl.metadata (15 kB)\n",
            "INFO: pip is still looking at multiple versions of typer to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading typer-0.17.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.17.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.16.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.15.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.14.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.13.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.13.0-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.4-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.2-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.1-py3-none-any.whl.metadata (15 kB)\n",
            "  Downloading typer-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting typer-slim==0.12.0 (from typer-slim[standard]==0.12.0->typer<1.0,>=0.12->gradio)\n",
            "  Downloading typer_slim-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting typer-cli==0.12.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading typer_cli-0.12.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "INFO: pip is looking at multiple versions of typer-slim to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting urduhack\n",
            "  Downloading urduhack-1.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading urduhack-1.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading urduhack-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading urduhack-1.0.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading urduhack-1.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "  Downloading urduhack-0.3.4-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting transformers~=2.10 (from urduhack)\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyconll~=2.2 (from urduhack)\n",
            "  Downloading pyconll-2.3.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting tokenizers==0.7.0 (from transformers~=2.10->urduhack)\n",
            "  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses (from transformers~=2.10->urduhack)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting urduhack\n",
            "  Downloading urduhack-0.3.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "INFO: pip is still looking at multiple versions of typer-slim to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.3.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.12/dist-packages (from urduhack) (2.19.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (3.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (3.15.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow~=2.2->urduhack) (0.5.3)\n",
            "  Downloading urduhack-0.3.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "INFO: pip is looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.2.7-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading urduhack-0.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
            "  Downloading urduhack-0.2.5-py3-none-any.whl.metadata (5.7 kB)\n",
            "  Downloading urduhack-0.2.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "  Downloading urduhack-0.2.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading urduhack-0.2.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading urduhack-0.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "INFO: pip is still looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.1.4-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urduhack-0.1.4-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ca9fbad395a3e17eb5df2e055af94a5732d0a94389f60f79657e6cc204ad252c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: urduhack, portalocker, colorama, sacrebleu, rouge-score\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 rouge-score-0.1.2 sacrebleu-2.5.1 urduhack-0.1.4\n"
          ]
        }
      ],
      "source": [
        "# ===========================================\n",
        "# 🚀 Urdu Conversational Chatbot using Transformer (from scratch)\n",
        "# ===========================================\n",
        "\n",
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio nltk datasets sentencepiece sacrebleu rouge-score gradio urduhack kaggle\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from datasets import load_dataset\n",
        "import gradio as gr\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "import urduhack\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 1: Load Dataset from Kaggle (Fixed)\n",
        "# ===========================================\n",
        "\n",
        "# Set up Kaggle API key (upload kaggle.json to Colab first)\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_KEY\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and unzip dataset\n",
        "!kaggle datasets download -d muhammadahmedansari/urdu-dataset-20000 -p ./data\n",
        "!unzip -q ./data/urdu-dataset-20000.zip -d ./data\n",
        "\n",
        "# -------------------------------------------\n",
        "# 🔹 Load the main TSV dataset\n",
        "# -------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "# Try reading TSV file safely\n",
        "file_path = \"./data/final_main_dataset.tsv\"\n",
        "data = pd.read_csv(file_path, sep=\"\\t\", quoting=3, on_bad_lines='skip')\n",
        "\n",
        "# Inspect available columns\n",
        "print(\"Columns:\", data.columns.tolist())\n",
        "print(\"Sample rows:\")\n",
        "print(data.head(3))\n",
        "\n",
        "# -------------------------------------------\n",
        "# 🔹 Identify text columns\n",
        "# -------------------------------------------\n",
        "# Most likely columns: 'input', 'response', 'question', 'answer', etc.\n",
        "# You can adjust column names below once you see printed names.\n",
        "\n",
        "if \"input\" in data.columns and \"response\" in data.columns:\n",
        "    data = data[[\"input\", \"response\"]]\n",
        "elif \"question\" in data.columns and \"answer\" in data.columns:\n",
        "    data = data[[\"question\", \"answer\"]]\n",
        "else:\n",
        "    # Fallback: use first two columns\n",
        "    data = data.iloc[:, :2]\n",
        "\n",
        "data.columns = [\"input_text\", \"target_text\"]\n",
        "\n",
        "# Drop NaNs and sample subset to fit in Colab memory\n",
        "data = data.dropna().sample(20000, random_state=42)\n",
        "\n",
        "print(\"✅ Loaded dataset with shape:\", data.shape)\n",
        "print(data.sample(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtoOs5G_A3hk",
        "outputId": "5a5570af-1873-4808-d3ca-fb9f199a8b8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/muhammadahmedansari/urdu-dataset-20000\n",
            "License(s): other\n",
            "urdu-dataset-20000.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace ./data/char_to_num_vocab.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/final_main_dataset.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/limited_wav_files/limited_wav_files/common_voice_ur_26562732.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/limited_wav_files/limited_wav_files/common_voice_ur_26562733.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ./data/limited_wav_files/limited_wav_files/common_voice_ur_26562734.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "n\n",
            "Columns: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment']\n",
            "Sample rows:\n",
            "                                           client_id  \\\n",
            "0  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "1  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "2  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "\n",
            "                           path                            sentence  up_votes  \\\n",
            "0  common_voice_ur_31771683.mp3  کبھی کبھار ہی خیالی پلاو بناتا ہوں         2   \n",
            "1  common_voice_ur_31771684.mp3   اور پھر ممکن ہے کہ پاکستان بھی ہو         2   \n",
            "2  common_voice_ur_31771685.mp3       یہ فیصلہ بھی گزشتہ دو سال میں         2   \n",
            "\n",
            "   down_votes       age gender accents  variant locale  segment  \n",
            "0           0  twenties   male     NaN      NaN     ur      NaN  \n",
            "1           1  twenties   male     NaN      NaN     ur      NaN  \n",
            "2           0  twenties   male     NaN      NaN     ur      NaN  \n",
            "✅ Loaded dataset with shape: (20000, 2)\n",
            "                                              input_text  \\\n",
            "12264  58fdc64b73679a0dea8d4bbcd1152d255d735fd0e48e2e...   \n",
            "16706  e53f84d151d6cc6d45a57decde08a99efe47d7751a4ca6...   \n",
            "19418  923cf937f8ab28ff4dab01c2b2e29c2b4d4afbc5fb7544...   \n",
            "\n",
            "                        target_text  \n",
            "12264  common_voice_ur_31973247.mp3  \n",
            "16706  common_voice_ur_31822532.mp3  \n",
            "19418  common_voice_ur_31822470.mp3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 1: Load and Prepare Urdu Text Dataset\n",
        "# ===========================================\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"./data/final_main_dataset.tsv\"\n",
        "data = pd.read_csv(file_path, sep=\"\\t\", quoting=3, on_bad_lines='skip')\n",
        "\n",
        "print(\"Columns:\", data.columns.tolist())\n",
        "print(\"Sample sentences:\")\n",
        "print(data[\"sentence\"].head(5))\n",
        "\n",
        "# Use only the text column\n",
        "sentences = data[\"sentence\"].dropna().astype(str).tolist()\n",
        "\n",
        "# Simulate conversational pairs:\n",
        "# e.g. sentence[i] -> sentence[i+1]\n",
        "input_texts = sentences[:-1]\n",
        "target_texts = sentences[1:]\n",
        "\n",
        "# Build DataFrame\n",
        "data = pd.DataFrame({\n",
        "    \"input_text\": input_texts,\n",
        "    \"target_text\": target_texts\n",
        "})\n",
        "\n",
        "# Take random subset to avoid memory overload\n",
        "data = data.sample(20000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"✅ Dataset prepared for chatbot training\")\n",
        "print(data.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcMT42v4Equk",
        "outputId": "f9259635-2a7a-412d-9257-505a3cf6d840"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'variant', 'locale', 'segment']\n",
            "Sample sentences:\n",
            "0                   کبھی کبھار ہی خیالی پلاو بناتا ہوں\n",
            "1                    اور پھر ممکن ہے کہ پاکستان بھی ہو\n",
            "2                        یہ فیصلہ بھی گزشتہ دو سال میں\n",
            "3                       ان کے بلے بازوں کے سامنے ہو گا\n",
            "4    آبی جانور میں بطخ بگلا اور دُوسْرا آبی پرندہ ش...\n",
            "Name: sentence, dtype: object\n",
            "✅ Dataset prepared for chatbot training\n",
            "                                  input_text  \\\n",
            "0                                یہ سب تسلیم   \n",
            "1      اور وہ تم پر اپنے نگران مقرر رکھتا ہے   \n",
            "2      سیاسی حقوق کے مطالبات کو جائز قرار دی   \n",
            "3                     لوگ چل کرگئےاورمرکرآئے   \n",
            "4  حضرت علی بن حسین رضی اللہ عنہ سے روایت ہے   \n",
            "\n",
            "                                      target_text  \n",
            "0                  انھیں ہیرو کا درجہ بھی دیا گیا  \n",
            "1            انسان کے اندر شعور کی عدالت قائم ہے۔  \n",
            "2  ہر فریق دوسرے کے لیے سِفارشات کی تجویز دیتا ہے  \n",
            "3                                بتانی شروع کردیں  \n",
            "4           ۔ جنہیں دبانے سے یہ ٹھنڈی ہوجاتے ہیں۔  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 2: Preprocessing (Final Fixed Version)\n",
        "# ===========================================\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download both required tokenizer models\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# ✅ Urdu normalization function (custom implementation)\n",
        "def normalize_urdu(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]', '', text)  # remove diacritics\n",
        "    text = re.sub(r'[اآإأٱ]', 'ا', text)  # normalize Alef\n",
        "    text = re.sub(r'[يىئ]', 'ی', text)    # normalize Yeh\n",
        "    text = re.sub(r'[ھہۀھٰ]', 'ہ', text)  # normalize Heh\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  # keep only Urdu chars\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # clean spaces\n",
        "    return text\n",
        "\n",
        "# Apply normalization\n",
        "data[\"input_text\"] = data[\"input_text\"].astype(str).apply(normalize_urdu)\n",
        "data[\"target_text\"] = data[\"target_text\"].astype(str).apply(normalize_urdu)\n",
        "\n",
        "# ✅ Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "data[\"input_tokens\"] = data[\"input_text\"].apply(tokenize)\n",
        "data[\"target_tokens\"] = data[\"target_text\"].apply(tokenize)\n",
        "\n",
        "# ✅ Build vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "all_tokens = [token for tokens in data[\"input_tokens\"] + data[\"target_tokens\"] for token in tokens]\n",
        "vocab = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + sorted(set(all_tokens))\n",
        "vocab2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2vocab = {i: w for w, i in vocab2idx.items()}\n",
        "\n",
        "print(\"✅ Preprocessing complete!\")\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"Sample tokens:\", data['input_tokens'].head(3).tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTk755_1A7zK",
        "outputId": "ac179559-53da-4e0e-e25b-6742339dfa26"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessing complete!\n",
            "Vocabulary size: 10897\n",
            "Sample tokens: [['یہ', 'سب', 'تسلیم'], ['اور', 'وہ', 'تم', 'پر', 'اپنے', 'نگران', 'مقرر', 'رکہتا', 'ہے'], ['سیاسی', 'حقوق', 'کے', 'مطالبات', 'کو', 'جایز', 'قرار', 'دی']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 3: Dataset & Dataloader\n",
        "# ===========================================\n",
        "\n",
        "MAX_LEN = 40\n",
        "\n",
        "def encode(tokens):\n",
        "    tokens = [\"<sos>\"] + tokens[:MAX_LEN-2] + [\"<eos>\"]\n",
        "    ids = [vocab2idx.get(t, vocab2idx[\"<unk>\"]) for t in tokens]\n",
        "    return ids + [vocab2idx[\"<pad>\"]] * (MAX_LEN - len(ids))\n",
        "\n",
        "class UrduChatDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.inputs = df[\"input_tokens\"].tolist()\n",
        "        self.targets = df[\"target_tokens\"].tolist()\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(encode(self.inputs[idx])), torch.tensor(encode(self.targets[idx]))\n",
        "\n",
        "dataset = UrduChatDataset(data)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32)"
      ],
      "metadata": {
        "id": "tpYsWpGHA_Tv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 4: Transformer Model (Fixed for Batch-First + Mask Shape)\n",
        "# ===========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, d_model]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerChatbot(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_heads=2, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        # ✅ batch_first=True is critical here\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, n_heads, dim_feedforward=512, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model, n_heads, dim_feedforward=512, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate autoregressive mask for decoder to prevent peeking ahead.\"\"\"\n",
        "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "        return mask  # shape: [sz, sz]\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src, trg: [batch, seq_len]\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        trg_emb = self.embedding(trg) * math.sqrt(self.d_model)\n",
        "\n",
        "        src_emb = self.pos_encoder(src_emb)\n",
        "        trg_emb = self.pos_encoder(trg_emb)\n",
        "\n",
        "        # ✅ Create target mask dynamically per batch\n",
        "        tgt_seq_len = trg_emb.size(1)\n",
        "        tgt_mask = self._generate_square_subsequent_mask(tgt_seq_len).to(trg.device)\n",
        "\n",
        "        # Encoder + Decoder\n",
        "        memory = self.encoder(src_emb)\n",
        "        output = self.decoder(trg_emb, memory, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Output projection\n",
        "        return self.fc_out(output)\n"
      ],
      "metadata": {
        "id": "qgi8msaMBCDX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 5: Training\n",
        "# ===========================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerChatbot(len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab2idx[\"<pad>\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, trg in tqdm(train_loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, len(vocab)), trg[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"urdu_chatbot.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2UE00hMBF1N",
        "outputId": "2f1a20ce-2b37-4b59-8042-a38d055806ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [12:52<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.4982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [12:09<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 5.5674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [12:18<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 4.9248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [12:12<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 4.3796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [11:54<00:00,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 3.8975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 6: Evaluation\n",
        "# ===========================================\n",
        "\n",
        "def evaluate_bleu(model, loader):\n",
        "    model.eval()\n",
        "    refs, hyps = [], []\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            src = src.to(device)\n",
        "            output = model(src, trg[:, :-1].to(device))\n",
        "            pred = output.argmax(-1)\n",
        "            for i in range(pred.size(0)):\n",
        "                ref = [idx2vocab[t.item()] for t in trg[i] if t.item() not in [0, 1, 2]]\n",
        "                hyp = [idx2vocab[t.item()] for t in pred[i] if t.item() not in [0, 1, 2]]\n",
        "                refs.append([' '.join(ref)])\n",
        "                hyps.append(' '.join(hyp))\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
        "    print(f\"BLEU: {bleu.score:.2f}\")\n",
        "\n",
        "evaluate_bleu(model, val_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rbN0t50BKIP",
        "outputId": "7140f00d-99ca-40aa-802f-28d117e4d366"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU: 1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 🔹 Step 7: Gradio Chat Interface\n",
        "# ===========================================\n",
        "\n",
        "def generate_reply(prompt):\n",
        "    model.eval()\n",
        "    tokens = tokenize(normalize_urdu(prompt))\n",
        "    ids = torch.tensor([encode(tokens)]).to(device)\n",
        "    trg = torch.tensor([[vocab2idx[\"<sos>\"]]]).to(device)\n",
        "    for _ in range(MAX_LEN):\n",
        "        out = model(ids, trg)\n",
        "        next_token = out.argmax(-1)[:, -1]\n",
        "        trg = torch.cat([trg, next_token.unsqueeze(0)], dim=1)\n",
        "        if next_token.item() == vocab2idx[\"<eos>\"]:\n",
        "            break\n",
        "    result = [idx2vocab[i.item()] for i in trg[0]][1:-1]\n",
        "    return \" \".join(result)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_reply,\n",
        "    inputs=gr.Textbox(label=\"🗨️ Urdu Input\", placeholder=\"اپنا سوال یہاں لکھیں...\", rtl=True),\n",
        "    outputs=gr.Textbox(label=\"🤖 Chatbot Reply\", rtl=True),\n",
        "    title=\"Urdu Transformer Chatbot\"\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "qQGDu5zABMso",
        "outputId": "d98d3f75-dbff-4e08-8df4-44344d484eab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8aa1a0523f28bd041e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8aa1a0523f28bd041e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}